{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fundamentals of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four Branches of Machine Learning\n",
    "Machine Learning algorithms generally fall into four broad categories. \n",
    "\n",
    "A **Supervised Learning** algorithm maps input data to known responses / targets. Applied examples of supervised learning include optical character recognition, speech recognition, image clasification and language translation. Beyond classification & regression, there are other sub-branches:\n",
    "- **Sequence Generation**: Given a picture, predict a caption describing it. Sequence generation can sometimes be reformulated as a series of classification problems\n",
    "- **Syntax Tree Prediction**: Given a sentence, predict its decomposition into a syntax tree\n",
    "- **Object Detection**: Given a picture, draw a bounding box around objects in the picture.\n",
    "- **Image Segmentation**: Given a picture, draw a pixel-level mask on a specific object\n",
    "\n",
    "An **Unsupervised Learning** finds interesting transformations of the input data without any help of any targets. Some uses of unsupservised learning are data visualisation, data compression and data denoising (de-noising). Unsupervised learning is the bread & butter of data analytics and is usually necessary to understand a dataset before attempting to solve a supervised-learning problem. Common sub-branches of unsupervised learning are dimensionality reduction and clustering.\n",
    "\n",
    "**Self-supervised Learning** is supervised learning without human-annotated labels. Labels are involved but they are generated from the input data, typically using a heuristic algorithm. Autoencoders are well-known self-supervised learning techniques.\n",
    "\n",
    "In **Reinforcement Learning**, an agent receives information about its an environment and learns to choose actions that will maximise some reward. For example, a neural network \"looks\" at a video game screen and outputs game actions in order to maximise its score. Currently, it mostly research and hasn't had significant practical successes beyond games. Some implementations of reinforcement learning are DeepMind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "In machine learning, the goal is to achieve models that **generalise**, that perform well on never-before-seen data. When a model perform well on the training set but not on the validation set, we say that it is **overfitting**. \n",
    "\n",
    "To generalise well, we split the data to <u>training sets</u>, <u>validation sets</u> and <u>test sets</u>. We train the model on the training set, evaluate on the validation set and do 1 final check on the test set. \n",
    "\n",
    "Developing a model always involves tuning its hyperparameters e.g. number of layers. Tuning hyperparameters is a form of learning too. In a train-validation 2-way split, what usually happens is we overfit the validation set. This is called information leaks into the validation set. The model performs artificially well on the test set. And this probably means the model might not generalise well on \"unseen data\". \n",
    "\n",
    "There are 2 common ways to split the data:\n",
    "- train-test split method (hold-out validation). Train on training data, and evaluate performance on test data.\n",
    "- k-fold cross validation method. For every iteration, use one of k partitions to test, while using the rest of the dataset to train.\n",
    "\n",
    "Beyond this, we can shuffle the dataset p-shuffles before doing the k-fold cross validation.\n",
    "\n",
    "<b>Things to be aware of when doing the splits</b>\n",
    "- Training set and test-set should be representative of the overall data. Usually it's good to stratify sample starting from the targets. If the original dataset has 2 labels split 40%-60%, then the training set and test set should both have this even split.\n",
    "- If we want to perform time series analysis then we should not shuffle before training\n",
    "- Ensure that as far as possible, during the split step, the results are that the train and test sets are disjoint. No data point exists in both training and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing, Feature Engineering, Feature Learning\n",
    "\n",
    "After selecting the model and knowing how to tune the model, we now turn to the data that is fed into the model. Data needs to be preprocessed so the model can consume it. Also, we want to transform / impute data to ensure the predictions are sound.\n",
    "\n",
    "#### <u>PREPROCESSING</u>\n",
    "<b>Vectorisation</b> - Most of the times before feeding to neural networks, we need to ensure data is in tensor form. This process is data vectorization. It is usually easy with numerical features but some transformations need to be done for text & image form data.\n",
    "\n",
    "<b>Normalisation</b> - In most datasets, different features have different ranges, some larger than others. So it's generally safe to perform normalisation on the data before feeding it to the network. Normalisation means the feature has a mean of 0 and a variance of 1.\n",
    "\n",
    "<b>Imputation / Handling Missing Values</b> - Sometimes, a feature might have some values missing. So it is good to handle missing values by performing imputation. It can be imputed with 0 for missing values. If you expect the test data to have missing values for a particular feature, it will be good to have that property in the training data too, so the network will know to drop that value during training.\n",
    "\n",
    "#### <u>FEATURE ENGINEERING</u>\n",
    "Feature engineering is the process of using domain knowledge to apply transformations of the data that make the learning easier (in this sense, find patterns more easily). High quality features allow you to solve ML models more elegantly with fewer resources. Also, they let you solve a problem with much less data. Deep learning models usually learn with more data available, so with fewer data points, high quality features is critical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
