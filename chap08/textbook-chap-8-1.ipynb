{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Generative Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The potential of AI to emulate human thought processes goes beyond passive tasks such as object recognition and mostly ractive tasks such as driving a car. It extends well into creative activities. In 2015 there was [Google DeepDream](https://ai.googleblog.com/2015/07/deepdream-code-example-for-visualizing.html) turning an image to a psychedelic mess of dog eyes and pareidolic objects. In 2016, a short movie Sunspring was directed using a script generated by an LSTM algorithm. Other artefacts generated by a neural network include a piece in music.\n",
    "\n",
    "A large part of artistic creation comes from simple pattern recognition and technical skill. Learning this pattern is what deep learning algorithms excel at. Machine learning models can learn the statistical <u>latent space</u> of images, music and stories, and they can <u>sample</u> from this space, creating new artworks with characteristics similar to those the model has seen in its training data.\n",
    "\n",
    "Here, we explore from various angles the potential of deep learning to augment artistic creation. Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation with LSTM\n",
    "\n",
    "Here, we will explore how recurrent neural networks can be used to generate sequence data. We'll use text generation as an example, but the same techniques can be generalized to any kind of sequence data: you can apply it to sequences of musical notes to generate new music, or timeseries of brush stroke data to generate paintings stroke by stroke, and so on.\n",
    "\n",
    "Sequence data generation is in no way limited to artistic content generation. It has been successfully applied to speech synthesis and dialogue generation for chatbots. The Smart Reply feature from Google in 2016, capably of automatically generating a selection of quick replies to emails or text messages are powered by similar techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we aim to train a network to predict the next token or next few tokens in a sequence, using the previous tokens as input. For example, given the input `the cat is on the ma`, the network is trained to predict the target `t`. Tokens are characters or words, and any network that can model the probability of the next token given the previous ones is a <b>language model</b>. A language model caputres the <u>latent space</u> of language: its statistical structure.\n",
    "\n",
    "Once you have trained a language model, you can <u>sample</u> from it - to get new sequences. You feed it an initial string of text (called <u>conditioning data</u>) and ask it to generate the next character or the next word, add the generated output back to the input data, and repeat the process many times. For this example, we feed it strings of $N$ characters extracted from a text corpus, and train it to predit character $N+1$. The output of the model will be a softmax over all possible characters. This LSTM is called a <u>character-level neural language model</u>.\n",
    "\n",
    "<img src=\"img81.png\" width=\"750\">\n",
    "\n",
    "When generating text, the way to choose the next character is very important. There is <u>greedy sampling</u>, choosing the most likely next character. But this results in repetitive, predictable strings that don't look like coherent language. The way to get more variety is to use <u>stochastic sampling</u>. So if say `e` has a 30% chance of being the next character, it will appear in 30% of samples.\n",
    "\n",
    "To control the amount of randomness, we use the `softmax temperature` parameter. On one extreme, each next value is equally likely to appear, resulting in maximum entropy and on the other, there is only 1 value and results in minimum entropy, so we want a sweet spot somewhere in between. So higher temperatures result in higher entropy and less predictability, and lower temperature results in lower entropy, more predictability.\n",
    "\n",
    "<img src=\"img82.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T07:23:12.599474Z",
     "start_time": "2020-06-17T07:23:10.668525Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, let's first get a large textual corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T07:23:12.609601Z",
     "start_time": "2020-06-17T07:23:12.601682Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ingestion\n",
    "###########\n",
    "txt = ''\n",
    "with open('nietzsche.txt', 'r') as f:\n",
    "    txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T07:23:12.615675Z",
     "start_time": "2020-06-17T07:23:12.613202Z"
    }
   },
   "outputs": [],
   "source": [
    "# For testing\n",
    "# print(len(t))\n",
    "# print(t[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T07:23:18.130906Z",
     "start_time": "2020-06-17T07:23:12.622161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of sentences = 200281\n",
      "no. of unique characters = 85\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "###############\n",
    "MAX_LENGTH, STEP = 60, 3\n",
    "\n",
    "sentences, next_chars = [], []\n",
    "\n",
    "# Iterate through the text, sampling every STEP\n",
    "# Obtain the sentences and the next character from the sentence\n",
    "for i in range(0, len(txt)-MAX_LENGTH, STEP):\n",
    "    sentences.append(txt[i:i+MAX_LENGTH])\n",
    "    next_chars.append(txt[i+MAX_LENGTH])\n",
    "print('no. of sentences = {:d}'.format(len(sentences)))\n",
    "\n",
    "# Convert the unique chars to a dictionary\n",
    "unique_chars = sorted(list(set(txt)))\n",
    "print('no. of unique characters = {:d}'.format(len(unique_chars)))\n",
    "char_indices = dict((c, unique_chars.index(c)) for c in unique_chars)\n",
    "\n",
    "# Vectorization\n",
    "x = np.zeros((len(sentences), MAX_LENGTH, len(unique_chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(unique_chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, ch in enumerate(sentence):\n",
    "        x[i, t, char_indices[ch]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is a single LSTM layer, followed by a Dense classifier and softmax over all possible characters. Note that there are also other models like 1D Convnets that can do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T07:23:18.497427Z",
     "start_time": "2020-06-17T07:23:18.132915Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "keras.backend.clear_session()\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.LSTM(128, input_shape=(MAX_LENGTH, len(unique_chars))))\n",
    "model.add(keras.layers.Dense(len(unique_chars), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.RMSprop(learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we \n",
    "\n",
    "1. draw from the model a probability distribution for next character, given the generated text available so far.\n",
    "2. reweight the distribution to a certain temperature\n",
    "3. sample the next character at random according to the reweighted distribution\n",
    "4. add the new character at the end of the available text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T07:23:18.506287Z",
     "start_time": "2020-06-17T07:23:18.499705Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds_float = np.asarray(preds).astype('float64')\n",
    "    preds_float = np.log(preds_float) / temperature\n",
    "    preds_float = np.exp(preds_float)\n",
    "    preds_float = preds_float / np.sum(preds_float)\n",
    "    probs = np.random.multinomial(1, preds_float, 1)\n",
    "    return np.argmax(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T07:23:18.520603Z",
     "start_time": "2020-06-17T07:23:18.509127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, ',': 7, '-': 8, '.': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, ':': 20, ';': 21, '=': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, '[': 50, ']': 51, '_': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, '¤': 79, '¦': 80, '©': 81, '«': 82, 'Ã': 83, '†': 84}\n"
     ]
    }
   ],
   "source": [
    "print(char_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T14:37:35.397704Z",
     "start_time": "2020-06-17T07:23:18.526345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### epoch = 1 , Training started...###\n",
      "1565/1565 [==============================] - 181s 116ms/step - loss: 2.0297\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "e, or somewhat feared! And pray, don't forget\n",
      "the garden, th\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 2 , Training started...###\n",
      "1565/1565 [==============================] - 204s 130ms/step - loss: 1.6703\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "in a world whose essence is Will\n",
      "to Power, may be reminded t\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 3 , Training started...###\n",
      "1565/1565 [==============================] - 271s 173ms/step - loss: 1.5772\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "e cases, only compulsorily, always without\n",
      "delight in 'the s\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 4 , Training started...###\n",
      "1565/1565 [==============================] - 287s 183ms/step - loss: 1.5278\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "on lonesome ice-lorn fell,\n",
      "     And unlearned Man and God an\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 5 , Training started...###\n",
      "1565/1565 [==============================] - 295s 188ms/step - loss: 1.4956\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "ly\n",
      "certainty, which exists or has existed, whether it be in \n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 6 , Training started...###\n",
      "1565/1565 [==============================] - 306s 196ms/step - loss: 1.4710\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "unity.\" La Rochefoucauld and\n",
      "those other French masters of s\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 7 , Training started...###\n",
      "1565/1565 [==============================] - 301s 192ms/step - loss: 1.4526\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "aid of himself--but whose curiosity\n",
      "always makes him \"come t\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 8 , Training started...###\n",
      "1565/1565 [==============================] - 297s 190ms/step - loss: 1.4386\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "rough teacher takes things seriously--and even\n",
      "himself--only\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 9 , Training started...###\n",
      "1565/1565 [==============================] - 304s 194ms/step - loss: 1.4263\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "tion of his \"Sentences and Moral Maxims\"\n",
      "has expressed it: \"\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 10 , Training started...###\n",
      "1565/1565 [==============================] - 322s 206ms/step - loss: 1.4154\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "possibly exist, that our Europe will yet number among her\n",
      "so\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 11 , Training started...###\n",
      "1565/1565 [==============================] - 308s 197ms/step - loss: 1.4050\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "f-satisfied satyr, but in every other sense he is the more o\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 12 , Training started...###\n",
      "1565/1565 [==============================] - 304s 194ms/step - loss: 1.3979\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "TODAY.\n",
      "\n",
      "241. We \"good Europeans,\" we also have hours when we\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 13 , Training started...###\n",
      "1565/1565 [==============================] - 297s 190ms/step - loss: 1.3919\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "er the RAFFINEMENTS of\n",
      "decadence--which, perhaps, feels itse\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 14 , Training started...###\n",
      "1565/1565 [==============================] - 299s 191ms/step - loss: 1.3849\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "rlude, the transition of Europe from\n",
      "Rousseau to Napoleon, a\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 15 , Training started...###\n",
      "1565/1565 [==============================] - 300s 192ms/step - loss: 1.3812\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "y, this universal\n",
      "inner \"desperateness\" of higher men, this \n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 16 , Training started...###\n",
      "1565/1565 [==============================] - 299s 191ms/step - loss: 1.3749\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "espondingly, the\n",
      "outer restlessness, the promiscuous flow of\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 17 , Training started...###\n",
      "1565/1565 [==============================] - 301s 192ms/step - loss: 1.3690\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "\n",
      "benevolent man, the helpful man, is duly styled \"good\". (At\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 18 , Training started...###\n",
      "1565/1565 [==============================] - 300s 191ms/step - loss: 1.3653\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "All of them are persons who have been vanquished\n",
      "and BROUGHT\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 19 , Training started...###\n",
      "1565/1565 [==============================] - 292s 186ms/step - loss: 1.3612\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "ety among Germans--pardon\n",
      "me for stating the fact that even \n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 20 , Training started...###\n",
      "1565/1565 [==============================] - 308s 197ms/step - loss: 1.3553\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "es of\n",
      "their actions, not the knowledge but the welfare of hu\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 21 , Training started...###\n",
      "1565/1565 [==============================] - 256s 164ms/step - loss: 1.3523\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "nd its problems would be stimulated, perhaps, even more.\n",
      "\n",
      "\n",
      "1\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 22 , Training started...###\n",
      "1565/1565 [==============================] - 289s 185ms/step - loss: 1.3482\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "e is injurious in itself;\" he knows that it is he himself\n",
      "on\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 23 , Training started...###\n",
      "1565/1565 [==============================] - 291s 186ms/step - loss: 1.3450\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      " pessimism, utilitarianism, or eudaemonism,\n",
      "all those modes \n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 24 , Training started...###\n",
      "1565/1565 [==============================] - 299s 191ms/step - loss: 1.3418\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "modes of thought--beyond good and evil, and no longer\n",
      "like B\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 25 , Training started...###\n",
      "1565/1565 [==============================] - 298s 190ms/step - loss: 1.3380\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "the deportment of love; but if I cease to love you\n",
      "my deport\n",
      "--- temperature=0.20 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 26 , Training started...###\n",
      "1565/1565 [==============================] - 283s 181ms/step - loss: 1.3360\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "n--think\n",
      "of Balzac, for instance,--unrestrained workers, alm\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 27 , Training started...###\n",
      "1565/1565 [==============================] - 269s 172ms/step - loss: 1.3342\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "nces, the entire history of the soul UP TO THE PRESENT TIME,\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 28 , Training started...###\n",
      "1565/1565 [==============================] - 270s 173ms/step - loss: 1.3301\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "g plebeianism, by which\n",
      "everything is rendered opaque and le\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 29 , Training started...###\n",
      "1565/1565 [==============================] - 274s 175ms/step - loss: 1.3291\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "ess in complaining, an effeminizing,\n",
      "which, with the aid of \n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryanlim/.pyenv/versions/3.7.2/envs/botanic/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### epoch = 30 , Training started...###\n",
      "1565/1565 [==============================] - 279s 178ms/step - loss: 1.3253\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "consequently also, a good deal more silent. It happens more \n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 31 , Training started...###\n",
      "1565/1565 [==============================] - 280s 179ms/step - loss: 1.3238\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "n that account, as instruments, they are\n",
      "far from being phil\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 32 , Training started...###\n",
      "1565/1565 [==============================] - 282s 180ms/step - loss: 1.3224\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      " SWEAT of the\n",
      "noble\"--but not at all as something easy and d\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 33 , Training started...###\n",
      "1565/1565 [==============================] - 283s 181ms/step - loss: 1.3185\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "nd mumbling and gaping and\n",
      "something uncanny going on. A phi\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 34 , Training started...###\n",
      "1565/1565 [==============================] - 283s 181ms/step - loss: 1.3154\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "ly succeed in DECEIVING with regard to such heredity.--And\n",
      "w\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 35 , Training started...###\n",
      "1565/1565 [==============================] - 286s 183ms/step - loss: 1.3134\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "xercised\n",
      "its acuteness and profundity has just been an occas\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 36 , Training started...###\n",
      "1565/1565 [==============================] - 286s 183ms/step - loss: 1.3113\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "edness of\n",
      "the learned man is now everywhere in full bloom, a\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 37 , Training started...###\n",
      "1565/1565 [==============================] - 2644s 2s/step - loss: 1.3113\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "s in\n",
      "woman\"--she has plenty of it!--is allowed to venture fo\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 38 , Training started...###\n",
      "1565/1565 [==============================] - 234s 150ms/step - loss: 1.3079\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "as if a new kind of explosive were being tried\n",
      "somewhere, a \n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 39 , Training started...###\n",
      "1565/1565 [==============================] - 817s 522ms/step - loss: 1.3074\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "\n",
      "Englishmen, and, as already remarked, in so far as they are\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 40 , Training started...###\n",
      "1565/1565 [==============================] - 1944s 1s/step - loss: 1.3068\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "found loves the mask: the profoundest things\n",
      "have a hatred e\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 41 , Training started...###\n",
      "1565/1565 [==============================] - 197s 126ms/step - loss: 1.3058\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      " have been in developing, since our\n",
      "intellectual and rationa\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 42 , Training started...###\n",
      "1565/1565 [==============================] - 233s 149ms/step - loss: 1.3032\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      " him who\n",
      "bothers himself with things that do not concern him\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 43 , Training started...###\n",
      "1565/1565 [==============================] - 246s 157ms/step - loss: 1.3023\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "s so imbedded in the passions, in language, in art, in\n",
      "relig\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 44 , Training started...###\n",
      "1565/1565 [==============================] - 259s 166ms/step - loss: 1.3003\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "cient world (although,\n",
      "as is appropriate in southern nations\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 45 , Training started...###\n",
      "1565/1565 [==============================] - 267s 170ms/step - loss: 1.2998\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "ognizable; there are actions of love and of an\n",
      "extravagant m\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 46 , Training started...###\n",
      "1565/1565 [==============================] - 248s 159ms/step - loss: 1.2972\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "g. You had to see with your own eyes the\n",
      "problem of classifi\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 47 , Training started...###\n",
      "1565/1565 [==============================] - 241s 154ms/step - loss: 1.2964\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      " if he were walling himself up alive in\n",
      "a mausoleum.\n",
      "\n",
      "[13] M\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 48 , Training started...###\n",
      "1565/1565 [==============================] - 244s 156ms/step - loss: 1.2972\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "uld, something uncommunicative and repulsive,\n",
      "which blows ch\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 49 , Training started...###\n",
      "1565/1565 [==============================] - 245s 157ms/step - loss: 1.2926\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "th good arguments.\" This was the real FALSENESS of that grea\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 50 , Training started...###\n",
      "1565/1565 [==============================] - 246s 157ms/step - loss: 1.2931\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "be rendered\n",
      "a matter of habit and therefore a pleasure.\n",
      "\n",
      "\n",
      "98\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 51 , Training started...###\n",
      "1565/1565 [==============================] - 250s 160ms/step - loss: 1.2913\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "the ascendancy for a time? It\n",
      "would be an error to consider \n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 52 , Training started...###\n",
      "1565/1565 [==============================] - 248s 158ms/step - loss: 1.2912\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "here.]\n",
      "\n",
      "191. The old theological problem of \"Faith\" and \"Kno\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 53 , Training started...###\n",
      "1565/1565 [==============================] - 248s 158ms/step - loss: 1.2900\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "cannot rid\n",
      "ourselves, we free spirits--well, we will labour \n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 54 , Training started...###\n",
      "1565/1565 [==============================] - 2511s 2s/step - loss: 1.2887\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "e considerations we can see how _late_ strict, logical thoug\n",
      "--- temperature=0.20 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 55 , Training started...###\n",
      "1565/1565 [==============================] - 192s 123ms/step - loss: 1.2879\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "ity and he\n",
      "soothes himself a little with the assertion that \n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 56 , Training started...###\n",
      "1565/1565 [==============================] - 199s 127ms/step - loss: 1.2877\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      " a phenomenon as the\n",
      "loftiest heroism of morality. It is alw\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 57 , Training started...###\n",
      "1565/1565 [==============================] - 239s 153ms/step - loss: 1.2865\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "m\n",
      "attained by a German, or almost always too late. The maste\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 58 , Training started...###\n",
      "1565/1565 [==============================] - 243s 156ms/step - loss: 1.2843\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      " not now regard it as a\n",
      "satisfaction, a relief, a deliveranc\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n",
      "### epoch = 59 , Training started...###\n",
      "1565/1565 [==============================] - 251s 161ms/step - loss: 1.2848\n",
      "### Training completed. ###\n",
      "--- Generating with seed:\n",
      "\n",
      "in the worst sense barbarous, asiatic, vulgar, un-Greek.\n",
      "\n",
      "\n",
      "\n",
      "--- temperature=0.20 ---\n",
      "\n",
      "--- temperature=0.50 ---\n",
      "\n",
      "--- temperature=1.00 ---\n",
      "\n",
      "--- temperature=1.20 ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vals = []\n",
    "for epoch in range(1,60):\n",
    "    # Train model\n",
    "    print('### epoch = %d , Training started...###' % epoch)\n",
    "    model_fp = \"language-model-attempt2-{:02d}.h5\".format(epoch)\n",
    "    m_callbacks = [keras.callbacks.ModelCheckpoint(model_fp, save_best_only=False),]\n",
    "    model.fit(x, y, batch_size=128, callbacks=m_callbacks, epochs=1)\n",
    "    print('### Training completed. ###')\n",
    "    \n",
    "    # Sample from text    \n",
    "    start_index = np.random.randint(0, len(txt) - MAX_LENGTH - 1)\n",
    "    generated_text = txt[start_index : start_index + MAX_LENGTH]\n",
    "    print('--- Generating with seed:')\n",
    "    print(generated_text)\n",
    "    original_text = generated_text\n",
    "    \n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('--- temperature={:.2f} ---'.format (temperature))\n",
    "        predicted_text = ''\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, MAX_LENGTH, len(unique_chars)))\n",
    "            \n",
    "            for t, c in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[c]] = 1\n",
    "            # Predict from the sample\n",
    "            ypred = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(ypred, temperature)\n",
    "            next_char = unique_chars[next_index]\n",
    "            predicted_text += next_char\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "#         print('--- Predicted:')\n",
    "#         print(original_text + predicted_text)\n",
    "        vals.append((epoch, temperature, original_text, original_text + predicted_text))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a low temperature value results in extremely repetitive and predictable text, but local structure is highly realistic. Most of the time the words are real English words. With higher temperatures, the generated text becomes more interesting, surprising, even creative. It sometimes invents completely new owrds that sound somewhat plausible. A good balance between learned structure and randomness is what makes generation interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T14:37:35.404415Z",
     "start_time": "2020-06-17T14:37:35.400749Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Train model\n",
    "# vals = []\n",
    "# epoch = 2\n",
    "# model_fp = \"language-model-{:02d}.h5\".format(epoch)\n",
    "# model = keras.models.load_model(model_fp)\n",
    "# # Sample from text    \n",
    "# start_index = np.random.randint(0, len(txt) - MAX_LENGTH - 1)\n",
    "# generated_text = txt[start_index : start_index + MAX_LENGTH]\n",
    "# print('--- Generating with seed:')\n",
    "# print(generated_text)\n",
    "# original_text = generated_text\n",
    "\n",
    "# for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "#     print('--- temperature={:.2f} ---'.format (temperature))\n",
    "#     predicted_text = ''\n",
    "#     for i in range(400):\n",
    "#         sampled = np.zeros((1, MAX_LENGTH, len(unique_chars)))\n",
    "\n",
    "#         for t, c in enumerate(generated_text):\n",
    "#             sampled[0, t, char_indices[c]] = 1\n",
    "#         # Predict from the sample\n",
    "#         ypred = model.predict(sampled, verbose=0)[0]\n",
    "#         next_index = sample(ypred, temperature)\n",
    "#         next_char = unique_chars[next_index]\n",
    "#         predicted_text += next_char\n",
    "#         generated_text += next_char\n",
    "#         generated_text = generated_text[1:]\n",
    "#     print('--- Predicted:')\n",
    "#     print(original_text + predicted_text)\n",
    "#     vals.append((epoch, temperature, original_text, original_text + predicted_text))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T14:37:35.430458Z",
     "start_time": "2020-06-17T14:37:35.414125Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(vals, columns=['epoch', 'temperature', 'seed', 'result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T14:37:35.453526Z",
     "start_time": "2020-06-17T14:37:35.435156Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df.to_csv('language_model_results.csv', sep='|', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
