{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Advanced Deep Learning Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Keras Functional API\n",
    "\n",
    "So far, the neural networks have been implemented using the `Sequential` model. This assumes that the model has <u>one and only one input</u> and <u>one and only one output</u>. Also, there is a linear stack of layers. Think of it as only 1 path with multiple layers.\n",
    "\n",
    "This is not ideal for some cases. Some networks have multiple independent inputs and some produce multiple outputs. Futhermore, some models have internal branching between layers that make them look like graphs rather than linear stacks of layers.\n",
    "\n",
    "Some tasks require <b>multimodal</b> inputs, that merge data from different input sources, processing each type of data using different kinds of neural layers. It's more ideal to predict jointly using different types of inputs (e.g. images & text) than learning different models for each output. Similarly, some models product multiple target attributes of input data. For example, jointly predicting the year of release and genre of a piece of writing.\n",
    "\n",
    "<img src=\"img71.png\" width=\"600\">\n",
    "<img src=\"img72.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are 3 examples of recent architectures that also don't obey the 1-input, 1-output, 1-stack architecture:\n",
    "\n",
    "- <b>Wide & Deep</b> neural network - This architecture connects all or part of the inputs directly to the output layer. With this architecture, it is possible to learn both deep patterns  (using the deep path) and simple rules (using the short path). More at [Wide & Deep Learning: Better Together with TensorFlow](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html)\n",
    "<img src=\"img3.png\" width=\"900\"/>\n",
    "\n",
    "- <b>Inception Family</b> - relies on inception modules, where the input is processed by several parallel convolutional branches, and their outputs are merged to a single tensor. More at [Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)\n",
    "\n",
    "- <b>Adding Residual Connections</b> - A residual connection of injecting previous representations into the downstream flow by adding a past output tensor to a later output tensor. More at [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "<img src=\"img73.png\" width=\"450\"/>\n",
    "\n",
    "To handle these use cases, and other cases, we cannot use the `Sequential` model but there is a more flexible way to use Keras - the <b>functional model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T03:11:26.486798Z",
     "start_time": "2020-06-15T03:11:26.482013Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Input, layers, models, backend, applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T02:17:53.192886Z",
     "start_time": "2020-06-15T02:17:53.179378Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ingestion\n",
    "###########\n",
    "(train_data, y_train), (test_data, y_test) = boston_housing.load_data()\n",
    "\n",
    "# Preprocessing\n",
    "###############\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(train_data)\n",
    "x_test = sc.transform(test_data)\n",
    "\n",
    "x_train__train, x_train__val, y_train__train, y_train__val = train_test_split(x_train, y_train, test_size=0.15,\n",
    "                                                                             random_state=0)\n",
    "NUM_FEATURES = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to the Functional API\n",
    "\n",
    "In the functional API, you directly manipulate tensors, and use layers as <u>functions</u> that take tensors and return tensors (hence, functional).\n",
    "\n",
    "#### Single Input, Single Output, One Linear Stack\n",
    "\n",
    "Let's build a side-by-side comparison of a simple model to tackle the **housing prices** regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T14:05:19.435914Z",
     "start_time": "2020-06-14T14:05:16.753250Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using models.Sequential()\n",
    "###########################\n",
    "backend.clear_session()\n",
    "m11 = models.Sequential() # Model\n",
    "m11.add(layers.Dense(32, activation='relu', \n",
    "                     input_shape=(NUM_FEATURES)))\n",
    "m11.add(layers.Dense(32, activation='relu'))\n",
    "m11.add(layers.Dense(1))\n",
    "print(m11.summary())\n",
    "\n",
    "m11.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) # Compile & Fit\n",
    "m11.fit(x_train__train, y_train__train, \n",
    "        epochs=20, batch_size=4,\n",
    "        validation_data= (x_train__val, y_train__val),\n",
    "       verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T14:05:22.420673Z",
     "start_time": "2020-06-14T14:05:19.438886Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using Functional API\n",
    "######################\n",
    "backend.clear_session()\n",
    "m12_input = Input(shape=NUM_FEATURES) # Model\n",
    "m12_l1 = layers.Dense(32, activation='relu')(m12_input)\n",
    "m12_l2 = layers.Dense(32, activation='relu')(m12_l1)\n",
    "m12_output = layers.Dense(1)(m12_l2)\n",
    "m12 = models.Model(m12_input, m12_output)\n",
    "print(m12.summary())\n",
    "m12.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) # Compile & Fit\n",
    "m12.fit(x_train__train, y_train__train, \n",
    "       epochs=20, batch_size=4,\n",
    "       validation_data=(x_train__val, y_train__val),\n",
    "       verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T14:05:22.553668Z",
     "start_time": "2020-06-14T14:05:22.423210Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict step\n",
    "print(m11.predict(x_train__val[:3]))\n",
    "print(m12.predict(x_train__val[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the backend, Keras retrieves every layer going from the inputs to the outputs to a graphs-like data structure, a `Model`. Of course, you need to ensure that there are intermediate layers between the inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T16:14:01.683382Z",
     "start_time": "2020-06-13T16:14:01.678891Z"
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "#### Multiple Inputs, Single Output\n",
    "Now, we shall build a model that have multiple inputs. Typically, for these models, there is a step to merge the different input branches that can combine several tensors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Example 1</b> - The **housing prices problem** now requires we use a subset of the features for one input and another subset of features for another. To do this, we need to make changes on <u>both the architecture</u> and the <u>input data</u>.\n",
    "\n",
    "For the architecture, the key features are:\n",
    "- 2 input layers\n",
    "- concatenate layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T14:04:11.913226Z",
     "start_time": "2020-06-14T14:04:11.843158Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate Model\n",
    "###################\n",
    "# Inputs\n",
    "input_layera = layers.Input(shape=(10,))\n",
    "input_layerb = layers.Input(shape=(7,))\n",
    "\n",
    "# Dense layers, Concatenate layer & Output layer is the same as previous complex workflows\n",
    "hidden_layer1 = layers.Dense(30, activation='relu')(input_layerb)\n",
    "hidden_layer2 = layers.Dense(30, activation='relu')(hidden_layer1)\n",
    "concat_layer = layers.Concatenate()([input_layera, hidden_layer2])\n",
    "output_layer = layers.Dense(1)(concat_layer)\n",
    "m21 = models.Model(inputs=[input_layera, input_layerb], outputs=output_layer)\n",
    "print(m21.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the input data, the key features are:\n",
    "\n",
    "- split the data to different subsets of features\n",
    "- in the `fit()` step, specify the inputs as a list of the 2 inputs, where the order is reflected in the functional API architecture. This needs to be the same in the `.evaluate()` and `predict()` step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T14:04:11.922174Z",
     "start_time": "2020-06-14T14:04:11.915921Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data for training model\n",
    "#################################\n",
    "inputa_cols = list(range(0,10))\n",
    "inputb_cols = [1,5,6,7,8,11,12]\n",
    "x_train__trainA = x_train__train[:,inputa_cols]\n",
    "x_train__trainB = x_train__train[:,inputb_cols]\n",
    "x_train__val_A = x_train__val[:,inputa_cols]\n",
    "x_train__val_B = x_train__val[:,inputb_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T14:04:11.928546Z",
     "start_time": "2020-06-14T14:04:11.925717Z"
    }
   },
   "outputs": [],
   "source": [
    "# For testing\n",
    "# print(x_train__trainA.shape)\n",
    "# print(x_train__trainB.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T14:04:13.866760Z",
     "start_time": "2020-06-14T14:04:12.493566Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train & Tune Model\n",
    "####################\n",
    "m21.compile(optimizer='sgd', loss='mean_squared_error', metrics=['mae'])\n",
    "m21.fit((x_train__trainA, x_train__trainB), y_train__train, epochs=20,\n",
    "           validation_data=((x_train__val_A, x_train__val_B), y_train__val), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T14:04:22.759811Z",
     "start_time": "2020-06-14T14:04:22.534797Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "###################\n",
    "x_testA = x_test[:,inputa_cols]\n",
    "x_testB = x_test[:,inputb_cols]\n",
    "\n",
    "# Evaluation\n",
    "m21.evaluate((x_testA, x_testB), y_test)\n",
    "\n",
    "# Prediction\n",
    "m21.predict((x_testA[:2], x_testB[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Example 2</b> - Consider a **Q&A problem** where there is a reference text and a question as the inputs, and the output is a one-word answer. Conceretely, there is a news article and \"country/person/incident\" as the question, and the outputs is a one-word answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T14:06:27.849626Z",
     "start_time": "2020-06-14T14:06:27.839502Z"
    }
   },
   "outputs": [],
   "source": [
    "TEXT_VOCAB_SIZE, QUESTION_VOCAB_SIZE, ANSWER_VOCAB_SIZE = 10000, 25, 500\n",
    "max_length, max_qn_length, max_ans_length = 100, 25, 5\n",
    "max_samples = 1000\n",
    "\n",
    "text_corpus = np.random.randint(1, TEXT_VOCAB_SIZE,\n",
    "                               size=(max_samples, max_length))\n",
    "questions_corpus = np.random.randint(1, QUESTION_VOCAB_SIZE,\n",
    "                               size=(max_samples, max_qn_length))\n",
    "answers_corpus = np.random.randint(0,ANSWER_VOCAB_SIZE,\n",
    "                                  size=(max_samples,))\n",
    "answers_corpus = to_categorical(answers_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T14:06:27.994517Z",
     "start_time": "2020-06-14T14:06:27.988677Z"
    }
   },
   "outputs": [],
   "source": [
    "print(text_corpus.shape)\n",
    "# print(text_corpus[:2])\n",
    "# print()\n",
    "print(questions_corpus.shape)\n",
    "# print(questions_corpus[:2])\n",
    "# print()\n",
    "print(answers_corpus.shape)\n",
    "# print(answers_corpus[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T14:06:30.213281Z",
     "start_time": "2020-06-14T14:06:29.666648Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "backend.clear_session()\n",
    "m31_corpus_input = Input(shape=(max_length,), dtype='int32')\n",
    "m31_qn_input = Input(shape=(max_qn_length,), dtype='int32')\n",
    "\n",
    "m31_corpus_emb = layers.Embedding(TEXT_VOCAB_SIZE, 64)(m31_corpus_input)\n",
    "m31_qn_emb = layers.Embedding(QUESTION_VOCAB_SIZE, 64)(m31_qn_input)\n",
    "\n",
    "m31_corpus_lstm = layers.LSTM(32)(m31_corpus_emb)\n",
    "m31_qn_lstm = layers.LSTM(32)(m31_qn_emb)\n",
    "\n",
    "m31_concat = layers.Concatenate()([m31_corpus_lstm, m31_qn_lstm])\n",
    "m31_ans = layers.Dense(ANSWER_VOCAB_SIZE, activation='softmax')(m31_concat)\n",
    "m31 = models.Model(inputs=[m31_corpus_input, m31_qn_input], outputs=m31_ans)\n",
    "print(m31.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T14:06:31.087988Z",
     "start_time": "2020-06-14T14:06:31.075564Z"
    }
   },
   "outputs": [],
   "source": [
    "m31.compile(optimizer='rmsprop', \n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T14:06:41.842844Z",
     "start_time": "2020-06-14T14:06:31.267970Z"
    }
   },
   "outputs": [],
   "source": [
    "m31.fit([text_corpus, questions_corpus], answers_corpus, \n",
    "        epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T14:08:30.204899Z",
     "start_time": "2020-06-14T14:08:30.154133Z"
    }
   },
   "outputs": [],
   "source": [
    "m31_pred = m31.predict([text_corpus[:2], questions_corpus[:2]])\n",
    "print(np.argmax(m31_pred[0]))\n",
    "print(np.argmax(m31_pred[1]))\n",
    "print(np.argmax(answers_corpus[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### Single Input, Multiple Outputs\n",
    "There are some other models that take one input and simultaneously predict different properties of the data.\n",
    "\n",
    "<b>Example 1</b> - Consider a **social media problem** where the network takes in a social media post as the input and predicts 3 outputs: the age, gender and income level of the poster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:31:27.049152Z",
     "start_time": "2020-06-14T15:31:27.033193Z"
    }
   },
   "outputs": [],
   "source": [
    "# Properties\n",
    "TEXT_VOCAB_SIZE = 50000\n",
    "NUM_GENDER_GROUPS, NUM_AGE_GROUPS = 2, 5\n",
    "text_length = 512 \n",
    "num_samples=1000\n",
    "\n",
    "# Prepare data\n",
    "text_corpus = np.random.randint(1, TEXT_VOCAB_SIZE,\n",
    "                               size=(num_samples, text_length))\n",
    "income_outcomes = np.random.random((num_samples,))\n",
    "gender_outcomes = np.random.randint(0, NUM_GENDER_GROUPS,\n",
    "                                   (num_samples,))\n",
    "age_outcomes = np.random.randint(0, NUM_AGE_GROUPS, \n",
    "                                (num_samples,))\n",
    "# IMPORTANT: When you do multiclass classification, you MUST \n",
    "# one-hot encode the results\n",
    "age_outcomes = to_categorical(age_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:31:27.564050Z",
     "start_time": "2020-06-14T15:31:27.373776Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "backend.clear_session()\n",
    "input_layer = Input(shape=(None,), dtype='int32', name='posts')\n",
    "\n",
    "embed_layer = layers.Embedding(\n",
    "    TEXT_VOCAB_SIZE, 256, input_length=text_length)(input_layer)\n",
    "stacked_layer = layers.Conv1D(16, 8, activation='relu')(embed_layer)\n",
    "stacked_layer = layers.MaxPooling1D(4)(stacked_layer)\n",
    "stacked_layer = layers.Conv1D(32, 8, activation='relu')(stacked_layer)\n",
    "stacked_layer = layers.GlobalMaxPooling1D()(stacked_layer)\n",
    "stacked_layer = layers.Dense(128, activation='relu')(stacked_layer)\n",
    "\n",
    "age_layer = layers.Dense(NUM_AGE_GROUPS, \n",
    "                         activation='softmax',\n",
    "                         name='age')(stacked_layer)\n",
    "income_layer = layers.Dense(1, name='income')(stacked_layer)\n",
    "gender_layer = layers.Dense(1, activation='sigmoid',\n",
    "                            name='gender')(stacked_layer)\n",
    "m41 = models.Model(input_layer, \n",
    "                   [age_layer, \n",
    "                       income_layer, gender_layer])\n",
    "print(m41.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:34:01.552601Z",
     "start_time": "2020-06-14T15:32:26.814162Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12/12 [==============================] - 5s 381ms/step - loss: 0.5147 - age_loss: 0.2895 - income_loss: 0.0655 - gender_loss: 0.1598 - age_acc: 1.0000 - income_mae: 0.1918 - gender_acc: 1.0000 - val_loss: 2.5632 - val_age_loss: 1.6878 - val_income_loss: 0.1281 - val_gender_loss: 0.7473 - val_age_acc: 0.1880 - val_income_mae: 0.3013 - val_gender_acc: 0.4920\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - 4s 371ms/step - loss: 0.2984 - age_loss: 0.1132 - income_loss: 0.1143 - gender_loss: 0.0709 - age_acc: 1.0000 - income_mae: 0.2675 - gender_acc: 1.0000 - val_loss: 2.6485 - val_age_loss: 1.7213 - val_income_loss: 0.1534 - val_gender_loss: 0.7737 - val_age_acc: 0.2040 - val_income_mae: 0.3289 - val_gender_acc: 0.4920\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - 4s 360ms/step - loss: 0.2077 - age_loss: 0.0610 - income_loss: 0.1068 - gender_loss: 0.0399 - age_acc: 1.0000 - income_mae: 0.2694 - gender_acc: 1.0000 - val_loss: 2.7219 - val_age_loss: 1.7540 - val_income_loss: 0.1652 - val_gender_loss: 0.8028 - val_age_acc: 0.1760 - val_income_mae: 0.3413 - val_gender_acc: 0.4920\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - 4s 341ms/step - loss: 0.1318 - age_loss: 0.0361 - income_loss: 0.0704 - gender_loss: 0.0253 - age_acc: 1.0000 - income_mae: 0.2164 - gender_acc: 1.0000 - val_loss: 2.7579 - val_age_loss: 1.7778 - val_income_loss: 0.1359 - val_gender_loss: 0.8442 - val_age_acc: 0.2000 - val_income_mae: 0.3100 - val_gender_acc: 0.4920\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - 4s 338ms/step - loss: 0.1034 - age_loss: 0.0223 - income_loss: 0.0652 - gender_loss: 0.0159 - age_acc: 1.0000 - income_mae: 0.2086 - gender_acc: 1.0000 - val_loss: 2.8352 - val_age_loss: 1.7906 - val_income_loss: 0.1950 - val_gender_loss: 0.8497 - val_age_acc: 0.2000 - val_income_mae: 0.3711 - val_gender_acc: 0.4920\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - 4s 343ms/step - loss: 0.1034 - age_loss: 0.0147 - income_loss: 0.0775 - gender_loss: 0.0112 - age_acc: 1.0000 - income_mae: 0.2295 - gender_acc: 1.0000 - val_loss: 2.8583 - val_age_loss: 1.8062 - val_income_loss: 0.1923 - val_gender_loss: 0.8598 - val_age_acc: 0.2160 - val_income_mae: 0.3681 - val_gender_acc: 0.4920\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - 4s 339ms/step - loss: 0.0770 - age_loss: 0.0109 - income_loss: 0.0576 - gender_loss: 0.0085 - age_acc: 1.0000 - income_mae: 0.1901 - gender_acc: 1.0000 - val_loss: 3.0238 - val_age_loss: 1.8187 - val_income_loss: 0.3213 - val_gender_loss: 0.8838 - val_age_acc: 0.2160 - val_income_mae: 0.4888 - val_gender_acc: 0.4920\n",
      "Epoch 8/20\n",
      "12/12 [==============================] - 4s 338ms/step - loss: 0.0584 - age_loss: 0.0082 - income_loss: 0.0435 - gender_loss: 0.0067 - age_acc: 1.0000 - income_mae: 0.1674 - gender_acc: 1.0000 - val_loss: 3.0147 - val_age_loss: 1.8290 - val_income_loss: 0.2912 - val_gender_loss: 0.8944 - val_age_acc: 0.2040 - val_income_mae: 0.4602 - val_gender_acc: 0.4920\n",
      "Epoch 9/20\n",
      "12/12 [==============================] - 4s 343ms/step - loss: 0.0733 - age_loss: 0.0067 - income_loss: 0.0615 - gender_loss: 0.0051 - age_acc: 1.0000 - income_mae: 0.2025 - gender_acc: 1.0000 - val_loss: 2.9717 - val_age_loss: 1.8344 - val_income_loss: 0.2468 - val_gender_loss: 0.8904 - val_age_acc: 0.2200 - val_income_mae: 0.4190 - val_gender_acc: 0.4920\n",
      "Epoch 10/20\n",
      "12/12 [==============================] - 4s 335ms/step - loss: 0.0708 - age_loss: 0.0054 - income_loss: 0.0611 - gender_loss: 0.0043 - age_acc: 1.0000 - income_mae: 0.2024 - gender_acc: 1.0000 - val_loss: 3.0259 - val_age_loss: 1.8552 - val_income_loss: 0.2273 - val_gender_loss: 0.9434 - val_age_acc: 0.2320 - val_income_mae: 0.4009 - val_gender_acc: 0.4920\n",
      "Epoch 11/20\n",
      "12/12 [==============================] - 4s 341ms/step - loss: 0.0522 - age_loss: 0.0044 - income_loss: 0.0441 - gender_loss: 0.0036 - age_acc: 1.0000 - income_mae: 0.1672 - gender_acc: 1.0000 - val_loss: 3.0921 - val_age_loss: 1.8588 - val_income_loss: 0.3361 - val_gender_loss: 0.8972 - val_age_acc: 0.2080 - val_income_mae: 0.5024 - val_gender_acc: 0.4920\n",
      "Epoch 12/20\n",
      "12/12 [==============================] - 4s 359ms/step - loss: 0.0485 - age_loss: 0.0038 - income_loss: 0.0417 - gender_loss: 0.0030 - age_acc: 1.0000 - income_mae: 0.1629 - gender_acc: 1.0000 - val_loss: 3.0827 - val_age_loss: 1.8684 - val_income_loss: 0.3057 - val_gender_loss: 0.9087 - val_age_acc: 0.2080 - val_income_mae: 0.4737 - val_gender_acc: 0.4920\n",
      "Epoch 13/20\n",
      "12/12 [==============================] - 4s 349ms/step - loss: 0.0499 - age_loss: 0.0034 - income_loss: 0.0437 - gender_loss: 0.0027 - age_acc: 1.0000 - income_mae: 0.1728 - gender_acc: 1.0000 - val_loss: 3.0846 - val_age_loss: 1.8844 - val_income_loss: 0.2742 - val_gender_loss: 0.9259 - val_age_acc: 0.2040 - val_income_mae: 0.4445 - val_gender_acc: 0.4920\n",
      "Epoch 14/20\n",
      "12/12 [==============================] - 4s 355ms/step - loss: 0.0449 - age_loss: 0.0029 - income_loss: 0.0396 - gender_loss: 0.0024 - age_acc: 1.0000 - income_mae: 0.1602 - gender_acc: 1.0000 - val_loss: 3.0655 - val_age_loss: 1.9047 - val_income_loss: 0.1848 - val_gender_loss: 0.9760 - val_age_acc: 0.2040 - val_income_mae: 0.3606 - val_gender_acc: 0.4920\n",
      "Epoch 15/20\n",
      "12/12 [==============================] - 4s 353ms/step - loss: 0.0262 - age_loss: 0.0024 - income_loss: 0.0218 - gender_loss: 0.0020 - age_acc: 1.0000 - income_mae: 0.1170 - gender_acc: 1.0000 - val_loss: 3.1933 - val_age_loss: 1.8824 - val_income_loss: 0.3273 - val_gender_loss: 0.9836 - val_age_acc: 0.1920 - val_income_mae: 0.4942 - val_gender_acc: 0.4920\n",
      "Epoch 16/20\n",
      "12/12 [==============================] - 5s 384ms/step - loss: 0.0368 - age_loss: 0.0020 - income_loss: 0.0331 - gender_loss: 0.0017 - age_acc: 1.0000 - income_mae: 0.1449 - gender_acc: 1.0000 - val_loss: 3.0940 - val_age_loss: 1.9134 - val_income_loss: 0.2289 - val_gender_loss: 0.9518 - val_age_acc: 0.2040 - val_income_mae: 0.4023 - val_gender_acc: 0.4920\n",
      "Epoch 17/20\n",
      "12/12 [==============================] - 4s 343ms/step - loss: 0.0447 - age_loss: 0.0020 - income_loss: 0.0411 - gender_loss: 0.0016 - age_acc: 1.0000 - income_mae: 0.1636 - gender_acc: 1.0000 - val_loss: 3.0723 - val_age_loss: 1.9246 - val_income_loss: 0.2014 - val_gender_loss: 0.9464 - val_age_acc: 0.2120 - val_income_mae: 0.3765 - val_gender_acc: 0.4920\n",
      "Epoch 18/20\n",
      "12/12 [==============================] - 4s 364ms/step - loss: 0.0332 - age_loss: 0.0019 - income_loss: 0.0296 - gender_loss: 0.0017 - age_acc: 1.0000 - income_mae: 0.1404 - gender_acc: 1.0000 - val_loss: 3.0638 - val_age_loss: 1.9148 - val_income_loss: 0.1910 - val_gender_loss: 0.9581 - val_age_acc: 0.2160 - val_income_mae: 0.3665 - val_gender_acc: 0.4920\n",
      "Epoch 19/20\n",
      "12/12 [==============================] - 5s 376ms/step - loss: 0.0306 - age_loss: 0.0016 - income_loss: 0.0278 - gender_loss: 0.0012 - age_acc: 1.0000 - income_mae: 0.1330 - gender_acc: 1.0000 - val_loss: 3.0469 - val_age_loss: 1.9636 - val_income_loss: 0.1681 - val_gender_loss: 0.9151 - val_age_acc: 0.2080 - val_income_mae: 0.3444 - val_gender_acc: 0.4920\n",
      "Epoch 20/20\n",
      "12/12 [==============================] - 4s 358ms/step - loss: 0.0282 - age_loss: 0.0015 - income_loss: 0.0255 - gender_loss: 0.0012 - age_acc: 1.0000 - income_mae: 0.1283 - gender_acc: 1.0000 - val_loss: 3.0316 - val_age_loss: 1.9473 - val_income_loss: 0.2225 - val_gender_loss: 0.8618 - val_age_acc: 0.1720 - val_income_mae: 0.3966 - val_gender_acc: 0.4920\n"
     ]
    }
   ],
   "source": [
    "m41.compile(optimizer='rmsprop', \n",
    "            loss={'age' : 'categorical_crossentropy', \n",
    "                  'income' : 'mse', \n",
    "                  'gender' : 'binary_crossentropy'},\n",
    "            metrics={'age' : 'acc', \n",
    "                     'income' : 'mae', \n",
    "                     'gender' : 'acc'})\n",
    "h41 = m41.fit(text_corpus, \n",
    "        {'age': age_outcomes, \n",
    "         'income' : income_outcomes, \n",
    "         'gender' : gender_outcomes},\n",
    "        epochs=20, batch_size=64, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:34:32.422636Z",
     "start_time": "2020-06-14T15:34:32.389973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>age_loss</th>\n",
       "      <th>income_loss</th>\n",
       "      <th>gender_loss</th>\n",
       "      <th>age_acc</th>\n",
       "      <th>income_mae</th>\n",
       "      <th>gender_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_age_loss</th>\n",
       "      <th>val_income_loss</th>\n",
       "      <th>val_gender_loss</th>\n",
       "      <th>val_age_acc</th>\n",
       "      <th>val_income_mae</th>\n",
       "      <th>val_gender_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.036835</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.033107</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.144874</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.094037</td>\n",
       "      <td>1.913367</td>\n",
       "      <td>0.228900</td>\n",
       "      <td>0.951769</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.402290</td>\n",
       "      <td>0.492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.044678</td>\n",
       "      <td>0.001954</td>\n",
       "      <td>0.041126</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.163551</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.072343</td>\n",
       "      <td>1.924612</td>\n",
       "      <td>0.201363</td>\n",
       "      <td>0.946367</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.376508</td>\n",
       "      <td>0.492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.033193</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.029634</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.140448</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.063819</td>\n",
       "      <td>1.914781</td>\n",
       "      <td>0.190971</td>\n",
       "      <td>0.958067</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.366486</td>\n",
       "      <td>0.492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.030628</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.027820</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.132962</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.046853</td>\n",
       "      <td>1.963621</td>\n",
       "      <td>0.168141</td>\n",
       "      <td>0.915091</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.344419</td>\n",
       "      <td>0.492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.028199</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.025508</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.128252</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.031599</td>\n",
       "      <td>1.947253</td>\n",
       "      <td>0.222498</td>\n",
       "      <td>0.861848</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.396601</td>\n",
       "      <td>0.492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  age_loss  income_loss  gender_loss  age_acc  income_mae  \\\n",
       "15  0.036835  0.002021     0.033107     0.001707      1.0    0.144874   \n",
       "16  0.044678  0.001954     0.041126     0.001598      1.0    0.163551   \n",
       "17  0.033193  0.001859     0.029634     0.001700      1.0    0.140448   \n",
       "18  0.030628  0.001592     0.027820     0.001216      1.0    0.132962   \n",
       "19  0.028199  0.001462     0.025508     0.001229      1.0    0.128252   \n",
       "\n",
       "    gender_acc  val_loss  val_age_loss  val_income_loss  val_gender_loss  \\\n",
       "15         1.0  3.094037      1.913367         0.228900         0.951769   \n",
       "16         1.0  3.072343      1.924612         0.201363         0.946367   \n",
       "17         1.0  3.063819      1.914781         0.190971         0.958067   \n",
       "18         1.0  3.046853      1.963621         0.168141         0.915091   \n",
       "19         1.0  3.031599      1.947253         0.222498         0.861848   \n",
       "\n",
       "    val_age_acc  val_income_mae  val_gender_acc  \n",
       "15        0.204        0.402290           0.492  \n",
       "16        0.212        0.376508           0.492  \n",
       "17        0.216        0.366486           0.492  \n",
       "18        0.208        0.344419           0.492  \n",
       "19        0.172        0.396601           0.492  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(h41.history).tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### DAG of Layers / Complex Architecture\n",
    "Beyond multiple inputs and multiple outputs, we can also build models with complex internal topology. Neural networks in Keras are allowed to be arbitrary directed acyclic graphs (DAGs) of layers. \n",
    "\n",
    "<b>Example 1</b> - Let's build a wide & deep network to tackle the **housing prices** problem. Take note of the comments describing each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T03:07:32.437659Z",
     "start_time": "2020-06-15T03:07:32.395340Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate Model\n",
    "###################\n",
    "backend.clear_session()\n",
    "# Input object. This is needed as we might have multiple inputs.\n",
    "m51_input_layer = layers.Input(shape=NUM_FEATURES)\n",
    "\n",
    "# Dense layer with 30 neurons & RELU activation. Notice it is called like a function,\n",
    "# passing in the input layer. \n",
    "m51_x = layers.Dense(30, activation='relu')(m51_input_layer)\n",
    "# Another Dense layer. Now, the first hidden layer is passed in.\n",
    "m51_x = layers.Dense(30, activation='relu')(m51_x)\n",
    "\n",
    "# Concatenate layer. concatenates the input & the output of the 2nd hidden layer\n",
    "m51_concat_layer = layers.Concatenate()([m51_input_layer, m51_x])\n",
    "\n",
    "# Output layer. Single neuron and no activation function.\n",
    "m51_output_layer = layers.Dense(1)(m51_concat_layer)\n",
    "\n",
    "# Finally, create the Keras model with this architecture.\n",
    "m51 = models.Model(inputs=[m51_input_layer], outputs=m51_output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T02:18:29.046412Z",
     "start_time": "2020-06-15T02:18:28.422211Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train & Tune Model\n",
    "####################\n",
    "m51.compile(optimizer='sgd', loss='mean_squared_error', metrics=['mae'])\n",
    "m51 = m51.fit(x_train, y_train,  epochs = 10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T02:18:36.016921Z",
     "start_time": "2020-06-15T02:18:36.014366Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "# m51.save('model0.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Example 2</b> - Inception is a popular type of network architecture for CNNs, developed in Google in 2013 - 2014. More at [Going Deeper with Convolutions](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43022.pdf). It consists of a stack of modeuls that themselves look like small independent networks, split into several parallel branches. The most basic form of an Inception module has three to four branches starting with a 1x1 convolution, then 3x3 convolution, then ending with the concatenation of the resulting features. This allows the network to separately learn spatial features and channel-wise features, which is more efficient than learning them jointly.\n",
    "\n",
    "There are several implementations of inception, but here is one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T02:28:27.957519Z",
     "start_time": "2020-06-15T02:28:27.854850Z"
    }
   },
   "outputs": [],
   "source": [
    "m61_input = Input(shape=(128,128,4))\n",
    "m61_bch_a = layers.Conv2D(128, 1, activation='relu', strides=2)(m61_input)\n",
    "\n",
    "m61_bch_b = layers.Conv2D(128, 1, activation='relu',)(m61_input)\n",
    "m61_bch_b = layers.Conv2D(128, 3, activation='relu', strides=2)(m61_bch_b)\n",
    "\n",
    "m61_bch_c = layers.AveragePooling2D(3, strides=2)(m61_input)\n",
    "m61_bch_c = layers.Conv2D(128, 3, activation='relu')(m61_bch_c)\n",
    "\n",
    "m61_bch_d = layers.Conv2D(128, 1, activation='relu')(m61_input)\n",
    "m61_bch_d = layers.Conv2D(128, 3, activation='relu')(m61_bch_d)\n",
    "m61_bch_d = layers.Conv2D(128, 3, activation='relu', strides=2)(m61_bch_d)\n",
    "\n",
    "output = layers.Concatenate([m61_bch_a, m61_bch_b, m61_bch_c, m61_bch_d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Example 3</b> - Residual connections are a common graph-like network component found in many post-2015 network architectures. They were introduced by Microsoft in their winning entry in the ILSVRC ImageNet challenge in 2015. They tackle two common problems that plague any large-scale deep learning model: vanishing gradients and representational bottlenecks. \n",
    "\n",
    "A residual connection consists of making the output of an earlier layer available as input to a later layer, effectively creating a shortcut in a sequential network. Rather than being concatenated to the later activation, the earlier output is summed with the later activation, which assumes that both activations are the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T02:39:48.380405Z",
     "start_time": "2020-06-15T02:39:48.377493Z"
    }
   },
   "outputs": [],
   "source": [
    "# When feature map sizes are the same, using identity residual connections\n",
    "###\n",
    "# x = ...\n",
    "# y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "# y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "# y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "# y = layers.add([y,x]) # Add the original x back to the output features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T02:39:48.548278Z",
     "start_time": "2020-06-15T02:39:48.545555Z"
    }
   },
   "outputs": [],
   "source": [
    "# When feature map sizes differ, using linear residual connection\n",
    "###\n",
    "# x = ...\n",
    "# y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "# y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "# y = layers.MaxPooling2D(2, strides=2)(y)\n",
    "# residual = layers.Conv2D(128, 1, strides=2, padding='same')(x)\n",
    "# y = layers.add([y, residual])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Representational Bottlenecks in Deep Learning</b> - In a `Sequential` model, each successive representation layer is built on top of the previous one, which means it only has access to information contained in the activation of the previous layer. If one layer is too small, then the model will be constrained by how much information can be crammed into the activations of this layer. Residual connections, by reinjecting earlier information downstream, partially solve this issue for deep learning models\n",
    "\n",
    "<b>Vanishing Gradients</b> - Backpropagation works by propagating a feedback signal from the output loss down to earlier layers. If this feedback signal has to be propagated through a deep stack of layers, the signal may become weak or lost entirely, rendering the network untrainable. This is known as vanishing gradients.\n",
    "\n",
    "This problem occurs both with deep networks and with recurrent networks over very long sequences - in both cases, a feedback signal must be propagated through a long series of operations. This is handled using the `LSTM` layer using the carry. Residual connections work in a similar way in feedforward deep networks, but they are even simpler: they introduce a purely linear information carry track parallel to the main layer stack, thus helpnig to propagate gradients through arbitrarily deep stacks of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### Multiple Inputs, Multiple Outputs\n",
    "\n",
    "For multiple outputs, you can use the following code snippets to help you.\n",
    "\n",
    "```python\n",
    "input_layera = tf_keras.layers.Input(shape=(10,))\n",
    "input_layerb = tf_keras.layers.Input(shape=(7,))\n",
    "\n",
    "hidden_layer1 = tf_keras.layers.Dense(30, activation='relu')(input_layerb)\n",
    "hidden_layer2 = tf_keras.layers.Dense(30, activation='relu')(hidden_layer1)\n",
    "concat_layer = tf_keras.layers.Concatenate()([input_layera, hidden_layer2])\n",
    "output_layer1 = tf_keras.layers.Dense(1)(concat_layer)\n",
    "output_layer2 = tf_keras.layers.Dense(1)(hidden_layer2) # Add this\n",
    "model3 = tf_keras.models.Model(inputs=[input_layera, input_layerb], \n",
    "                               outputs=[output_layer1, output_layer2]) # Change this\n",
    "```\n",
    "\n",
    "When compiling the model, use different metrics for different outputs\n",
    "\n",
    "```python\n",
    "model3.compile(optimizer='sgd', loss='mean_squared_error', metrics=['mae', 'mse'])\n",
    "```\n",
    "\n",
    "When evaluating the model, Keras returns the total loss, as well as the individual losses\n",
    "```python\n",
    "model3.evaluate((x_testA, x_testB), y_test)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### Layer Weight Sharing\n",
    "\n",
    "There are other uses of the functional API. One is the ability to resue a layer instance several times. When you call a layer instance twice, instead of instantiating a new layer for each call, you reuse the same weights with every call. This allows you to build models with shared branches - several branches that all share the same knowledge and perform the same operations. That is they share the same representations and learn these representations simultaneously for different sets of inputs.\n",
    "\n",
    "<b>Example 1</b> - Consider a model that assess the semantic similarity between two sentences. The model has two inputs and outputs a score between 0 and 1 - 0 being no similarity while 1 being complete similarity.\n",
    "\n",
    "Here, the two input sentences are interchangeable. So it wouldn't make sense to learn two independent models for prcessing each input sentence. Rather, you want to process both with a single LSTM layer. The representations of this LSTM layer are learned based on both inputs simultaneously. This is called the <b>Siamese LSTM</b> or <b>shared LSTM</b> model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T03:07:38.390940Z",
     "start_time": "2020-06-15T03:07:37.878835Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 128)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 128)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 32)           20608       input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64)           0           lstm[0][0]                       \n",
      "                                                                 lstm[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            65          concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 20,673\n",
      "Trainable params: 20,673\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "backend.clear_session()\n",
    "lstm = layers.LSTM(32)\n",
    "\n",
    "left_input = Input(shape=(None, 128))\n",
    "left_output = lstm(left_input)\n",
    "right_input = Input(shape=(None, 128))\n",
    "right_output = lstm(right_input)\n",
    "\n",
    "merged = layers.Concatenate()([left_output, right_output])\n",
    "predictions = layers.Dense(1, activation='sigmoid')(merged)\n",
    "model = models.Model([left_input, right_input], predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### Models as Layers\n",
    "\n",
    "Models can be used as you'd use layers. This means you can call a model on an input tensor and retrieve an output tensor:\n",
    "\n",
    "```python\n",
    "y = model(x)\n",
    "```\n",
    "\n",
    "If the model has multiple input tensors and multiple output tensors, it should be called with a list of tensors:\n",
    "```python\n",
    "y1, y2 = model([x1, x2])\n",
    "```\n",
    "\n",
    "When you call a model instance, you're reusing the weights of the model - exactly like what happens when you call a layer instance or a model instance. Calling an instance, whether it's a layer instance or model instance, will always reuse the existing learned representations of the instance, which is intuitive.\n",
    "\n",
    "We have seen something like this before, using the convolutional base of trained networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T03:13:01.671560Z",
     "start_time": "2020-06-15T03:12:56.600295Z"
    }
   },
   "outputs": [],
   "source": [
    "# xception_base = applications.Xception(weights=None, include_top=False)\n",
    "\n",
    "# left_input = Input(shape=(250,250,3))\n",
    "# right_input = Input(shape=(250,250,3))\n",
    "\n",
    "# left_features = xception_base(left_input)\n",
    "# right_features = xception_base(right_input)\n",
    "\n",
    "# merged_features = layers.concatenate([left_features, right_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To wrap up, the functional API achieves the following:\n",
    "\n",
    "- use complex architectures, beyond the `Sequential` model\n",
    "- build architectures with multiple inputs or with multiple outputs\n",
    "- reuse the weights of a layer or model across different processing branches, by calling the same layer or model instance several times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Building Dynamic Models Using the Subclassing API\n",
    "\n",
    "To add flexibility, we can use the Subclassing API to subclass the Model and create the layers needed.\n",
    "\n",
    "Here, we separate the creating of the layers from their usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:34:31.038033Z",
     "start_time": "2020-06-01T04:34:31.030801Z"
    }
   },
   "outputs": [],
   "source": [
    "class WideAndDeepModel(tf_keras.models.Model):\n",
    "    def __init__(self, units=30, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_layer1 = tf_keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden_layer2 = tf_keras.layers.Dense(units, activation=activation)\n",
    "        self.output_layer = tf_keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        inputa, inputb = inputs\n",
    "        hidden1 = self.hidden_layer1(inputb)\n",
    "        hidden2 = self.hidden_layer2(hidden1)\n",
    "        conct = tf_keras.layers.Concatenate()([inputa, hidden2])\n",
    "        ouptt = self.output_layer(conct)\n",
    "        return ouptt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:34:32.456367Z",
     "start_time": "2020-06-01T04:34:31.040944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load & Train model\n",
    "model3 = WideAndDeepModel(30, 'relu')\n",
    "model3.compile(optimizer='sgd', loss='mean_squared_error', metrics=['mae'])\n",
    "model3.fit((x_train__trainA, x_train__trainB), y_train__train, epochs=20,\n",
    "           validation_data=((x_train__val_A, x_train__val_B), y_train__val), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:34:32.572023Z",
     "start_time": "2020-06-01T04:34:32.459904Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate & Predict\n",
    "model3.evaluate((x_testA, x_testB), y_test)\n",
    "model3.predict((x_testA[:2], x_testB[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving & Restoring a Model\n",
    "\n",
    "This is useful when models take a long time to train or when you need access to a previously trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:34:32.594420Z",
     "start_time": "2020-06-01T04:34:32.574316Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving a model\n",
    "# model1.save('model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:34:32.719754Z",
     "start_time": "2020-06-01T04:34:32.596313Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load & Predict\n",
    "# model1ld = tf_keras.models.load_model('model3.h5')\n",
    "# model1ld.predict((x_testA[10:15], x_testB[10:15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Readings:\n",
    "\n",
    "- (1)  https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html\n",
    "- (2)  https://github.com/lutzroeder/Netron"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
